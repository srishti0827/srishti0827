class MyLinearRegression:
    def __init__(self, weight=9.1,bias=3,learning_rate=0.01,
                 iterations=5):
        self.weight = weight
        self.bias = bias
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.cost_trend = []
        self.cost = 0

    def predict(self, x):
        predicted_set = []
        for i in range(len(x)):
            predicted_value = self.weight * x[i] + self.bias
            predicted_set.append(predicted_value)
        return predicted_set

    def cost_function(self, x, y):
        count = len(x)
        total_error = 0.0
        for i in range(count):
            total_error += (y[i] - (self.weight * x[i] +
                            self.bias)) ** 2
        return float(total_error) / (2 * count)

    def update_weights(self, x, y):
        weight_deriv = 0
        bias_deriv = 0
        count = len(x)

        for i in range(count):
            # Calculate partial derivatives
            # -2x(y - (mx + b))
            weight_deriv += -2 * x[i] * (y[i] -(self.weight * x[i] + self.bias))

            # -2(y - (mx + b))
            bias_deriv += -2 * (y[i] - (self.weight * x[i] +
                                self.bias))

        # We subtract because the derivatives point in direction of steepest
        # ascent
        self.weight -= (weight_deriv / count) * self.learning_rate
        self.bias -= (bias_deriv / count) * self.learning_rate

    def train(self, x, y):
        for i in range(self.iterations):
            self.update_weights(x, y)
            # Calculating cost
            self.cost = self.cost_function(x, y)
            self.cost_trend.append(self.cost)
           # if i % 10000 == 0:
            print("Iteration: {}\t Weight: {}\t Bias: {}\t Cost: {}".format(i, self.weight, self.bias, self.cost))
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# intialise data of lists. 
data = {'Hours':[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8], 
        'Scores':[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]} 
  
# Create DataFrame 
studentscores = pd.DataFrame(data) 
  
# Print the output. 
studentscores
Hours	Scores
0	2.5	21
1	5.1	47
2	3.2	27
3	8.5	75
4	3.5	30
5	1.5	20
6	9.2	88
7	5.5	60
8	8.3	81
9	2.7	25
10	7.7	85
11	5.9	62
12	4.5	41
13	3.3	42
14	1.1	17
15	8.9	95
16	2.5	30
17	1.9	24
18	6.1	67
19	7.4	69
20	2.7	30
21	4.8	54
22	3.8	35
23	6.9	76
24	7.8	86
x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8] 
y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]
plt.scatter(x,y,s=10)
plt.xlabel('X')
plt.ylabel('y')
plt.show()

#from my_linear_regression import MyLinearRegression
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

# Importing the dataset

X = studentscores.iloc[:, : -1].values
y = studentscores.iloc[:, -1].values
X,y
(array([[2.5],
        [5.1],
        [3.2],
        [8.5],
        [3.5],
        [1.5],
        [9.2],
        [5.5],
        [8.3],
        [2.7],
        [7.7],
        [5.9],
        [4.5],
        [3.3],
        [1.1],
        [8.9],
        [2.5],
        [1.9],
        [6.1],
        [7.4],
        [2.7],
        [4.8],
        [3.8],
        [6.9],
        [7.8]]),
 array([21, 47, 27, 75, 30, 20, 88, 60, 81, 25, 85, 62, 41, 42, 17, 95, 30,
        24, 67, 69, 30, 54, 35, 76, 86]))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/8, random_state=0)

# Fitting Simple Linear Regression to the Training set
regressor = MyLinearRegression()
regressor.train(X_train, y_train)
print('Weight: ' + str(regressor.weight) + ' Bias: ' + str(regressor.bias))

# Predicting the Test set results
y_pred = regressor.predict(X_test)
Iteration: 0	 Weight: [9.55121619]	 Bias: [3.06726667]	 Cost: 15.58358879961457
Iteration: 1	 Weight: [9.69129389]	 Bias: [3.08561692]	 Cost: 14.929929711121483
Iteration: 2	 Weight: [9.73504726]	 Bias: [3.08883198]	 Cost: 14.866690885067442
Iteration: 3	 Weight: [9.74897899]	 Bias: [3.08736988]	 Cost: 14.860042724861309
Iteration: 4	 Weight: [9.75367713]	 Bias: [3.08446822]	 Cost: 14.858822296580565
Iteration: 5	 Weight: [9.7555154]	 Bias: [3.08112928]	 Cost: 14.858126052286119
Iteration: 6	 Weight: [9.756467]	 Bias: [3.07766331]	 Cost: 14.857484002514454
Iteration: 7	 Weight: [9.75714279]	 Bias: [3.07416633]	 Cost: 14.85685107586143
Iteration: 8	 Weight: [9.75773188]	 Bias: [3.07066804]	 Cost: 14.85622292579377
Iteration: 9	 Weight: [9.75829284]	 Bias: [3.06717762]	 Cost: 14.855599109084725
Iteration: 10	 Weight: [9.7588438]	 Bias: [3.06369786]	 Cost: 14.854979556787455
Iteration: 11	 Weight: [9.75939036]	 Bias: [3.06022961]	 Cost: 14.854364236007314
Iteration: 12	 Weight: [9.75993429]	 Bias: [3.0567731]	 Cost: 14.853753117484446
Iteration: 13	 Weight: [9.76047611]	 Bias: [3.05332837]	 Cost: 14.853146172485689
Iteration: 14	 Weight: [9.761016]	 Bias: [3.04989542]	 Cost: 14.852543372505414
Iteration: 15	 Weight: [9.76155403]	 Bias: [3.0464742]	 Cost: 14.85194468923566
Iteration: 16	 Weight: [9.7620902]	 Bias: [3.04306469]	 Cost: 14.851350094562084
Iteration: 17	 Weight: [9.76262454]	 Bias: [3.03966684]	 Cost: 14.850759560562384
Iteration: 18	 Weight: [9.76315706]	 Bias: [3.03628061]	 Cost: 14.85017305950494
Iteration: 19	 Weight: [9.76368775]	 Bias: [3.03290596]	 Cost: 14.849590563847507
Iteration: 20	 Weight: [9.76421662]	 Bias: [3.02954286]	 Cost: 14.849012046235973
Iteration: 21	 Weight: [9.76474369]	 Bias: [3.02619126]	 Cost: 14.848437479503005
Iteration: 22	 Weight: [9.76526895]	 Bias: [3.02285113]	 Cost: 14.84786683666681
Iteration: 23	 Weight: [9.76579242]	 Bias: [3.01952241]	 Cost: 14.847300090929851
Iteration: 24	 Weight: [9.76631409]	 Bias: [3.01620509]	 Cost: 14.846737215677633
Iteration: 25	 Weight: [9.76683398]	 Bias: [3.01289911]	 Cost: 14.846178184477399
Iteration: 26	 Weight: [9.7673521]	 Bias: [3.00960444]	 Cost: 14.84562297107689
Iteration: 27	 Weight: [9.76786844]	 Bias: [3.00632104]	 Cost: 14.845071549403174
Iteration: 28	 Weight: [9.76838301]	 Bias: [3.00304887]	 Cost: 14.84452389356134
Iteration: 29	 Weight: [9.76889583]	 Bias: [2.99978789]	 Cost: 14.843979977833339
Iteration: 30	 Weight: [9.76940689]	 Bias: [2.99653807]	 Cost: 14.843439776676787
Iteration: 31	 Weight: [9.7699162]	 Bias: [2.99329936]	 Cost: 14.842903264723677
Iteration: 32	 Weight: [9.77042377]	 Bias: [2.99007174]	 Cost: 14.84237041677928
Iteration: 33	 Weight: [9.77092961]	 Bias: [2.98685515]	 Cost: 14.841841207820943
Iteration: 34	 Weight: [9.77143371]	 Bias: [2.98364956]	 Cost: 14.841315612996885
Iteration: 35	 Weight: [9.77193609]	 Bias: [2.98045494]	 Cost: 14.840793607625043
Iteration: 36	 Weight: [9.77243675]	 Bias: [2.97727124]	 Cost: 14.840275167191919
Iteration: 37	 Weight: [9.7729357]	 Bias: [2.97409844]	 Cost: 14.839760267351428
Iteration: 38	 Weight: [9.77343295]	 Bias: [2.97093649]	 Cost: 14.83924888392376
Iteration: 39	 Weight: [9.77392849]	 Bias: [2.96778535]	 Cost: 14.838740992894207
Iteration: 40	 Weight: [9.77442233]	 Bias: [2.96464499]	 Cost: 14.838236570412104
Iteration: 41	 Weight: [9.77491449]	 Bias: [2.96151538]	 Cost: 14.837735592789633
Iteration: 42	 Weight: [9.77540497]	 Bias: [2.95839646]	 Cost: 14.83723803650078
Iteration: 43	 Weight: [9.77589376]	 Bias: [2.95528822]	 Cost: 14.836743878180172
Iteration: 44	 Weight: [9.77638089]	 Bias: [2.95219061]	 Cost: 14.836253094622014
Iteration: 45	 Weight: [9.77686634]	 Bias: [2.94910359]	 Cost: 14.835765662779014
Iteration: 46	 Weight: [9.77735014]	 Bias: [2.94602714]	 Cost: 14.83528155976123
Iteration: 47	 Weight: [9.77783228]	 Bias: [2.9429612]	 Cost: 14.834800762835087
Iteration: 48	 Weight: [9.77831278]	 Bias: [2.93990576]	 Cost: 14.834323249422239
Iteration: 49	 Weight: [9.77879163]	 Bias: [2.93686076]	 Cost: 14.833848997098537
Iteration: 50	 Weight: [9.77926884]	 Bias: [2.93382618]	 Cost: 14.83337798359299
Iteration: 51	 Weight: [9.77974442]	 Bias: [2.93080198]	 Cost: 14.83291018678668
Iteration: 52	 Weight: [9.78021837]	 Bias: [2.92778812]	 Cost: 14.832445584711754
Iteration: 53	 Weight: [9.78069071]	 Bias: [2.92478458]	 Cost: 14.831984155550405
Iteration: 54	 Weight: [9.78116142]	 Bias: [2.9217913]	 Cost: 14.831525877633798
Iteration: 55	 Weight: [9.78163053]	 Bias: [2.91880827]	 Cost: 14.831070729441095
Iteration: 56	 Weight: [9.78209803]	 Bias: [2.91583544]	 Cost: 14.83061868959842
Iteration: 57	 Weight: [9.78256393]	 Bias: [2.91287277]	 Cost: 14.830169736877892
Iteration: 58	 Weight: [9.78302824]	 Bias: [2.90992024]	 Cost: 14.829723850196547
Iteration: 59	 Weight: [9.78349096]	 Bias: [2.90697781]	 Cost: 14.829281008615494
Iteration: 60	 Weight: [9.7839521]	 Bias: [2.90404545]	 Cost: 14.828841191338737
Iteration: 61	 Weight: [9.78441166]	 Bias: [2.90112311]	 Cost: 14.828404377712394
Iteration: 62	 Weight: [9.78486965]	 Bias: [2.89821077]	 Cost: 14.827970547223545
Iteration: 63	 Weight: [9.78532607]	 Bias: [2.8953084]	 Cost: 14.827539679499452
Iteration: 64	 Weight: [9.78578093]	 Bias: [2.89241595]	 Cost: 14.82711175430643
Iteration: 65	 Weight: [9.78623424]	 Bias: [2.88953339]	 Cost: 14.82668675154902
Iteration: 66	 Weight: [9.78668599]	 Bias: [2.88666069]	 Cost: 14.826264651269009
Iteration: 67	 Weight: [9.7871362]	 Bias: [2.88379782]	 Cost: 14.825845433644444
Iteration: 68	 Weight: [9.78758487]	 Bias: [2.88094474]	 Cost: 14.825429078988774
Iteration: 69	 Weight: [9.78803201]	 Bias: [2.87810143]	 Cost: 14.825015567749878
Iteration: 70	 Weight: [9.78847761]	 Bias: [2.87526783]	 Cost: 14.824604880509188
Iteration: 71	 Weight: [9.78892169]	 Bias: [2.87244393]	 Cost: 14.824196997980696
Iteration: 72	 Weight: [9.78936426]	 Bias: [2.86962969]	 Cost: 14.823791901010184
Iteration: 73	 Weight: [9.78980531]	 Bias: [2.86682507]	 Cost: 14.823389570574173
Iteration: 74	 Weight: [9.79024485]	 Bias: [2.86403005]	 Cost: 14.822989987779145
Iteration: 75	 Weight: [9.79068288]	 Bias: [2.86124459]	 Cost: 14.822593133860593
Iteration: 76	 Weight: [9.79111942]	 Bias: [2.85846865]	 Cost: 14.822198990182152
Iteration: 77	 Weight: [9.79155446]	 Bias: [2.85570221]	 Cost: 14.821807538234747
Iteration: 78	 Weight: [9.79198802]	 Bias: [2.85294524]	 Cost: 14.821418759635698
Iteration: 79	 Weight: [9.7924201]	 Bias: [2.85019769]	 Cost: 14.821032636127887
Iteration: 80	 Weight: [9.79285069]	 Bias: [2.84745954]	 Cost: 14.820649149578834
Iteration: 81	 Weight: [9.79327981]	 Bias: [2.84473076]	 Cost: 14.820268281979937
Iteration: 82	 Weight: [9.79370747]	 Bias: [2.84201131]	 Cost: 14.819890015445559
Iteration: 83	 Weight: [9.79413366]	 Bias: [2.83930116]	 Cost: 14.81951433221221
Iteration: 84	 Weight: [9.7945584]	 Bias: [2.83660029]	 Cost: 14.819141214637703
Iteration: 85	 Weight: [9.79498168]	 Bias: [2.83390865]	 Cost: 14.818770645200377
Iteration: 86	 Weight: [9.79540351]	 Bias: [2.83122622]	 Cost: 14.818402606498182
Iteration: 87	 Weight: [9.7958239]	 Bias: [2.82855296]	 Cost: 14.818037081247942
Iteration: 88	 Weight: [9.79624286]	 Bias: [2.82588885]	 Cost: 14.817674052284493
Iteration: 89	 Weight: [9.79666038]	 Bias: [2.82323385]	 Cost: 14.817313502559912
Iteration: 90	 Weight: [9.79707647]	 Bias: [2.82058793]	 Cost: 14.816955415142708
Iteration: 91	 Weight: [9.79749114]	 Bias: [2.81795106]	 Cost: 14.816599773217
Iteration: 92	 Weight: [9.79790439]	 Bias: [2.81532321]	 Cost: 14.816246560081755
Iteration: 93	 Weight: [9.79831623]	 Bias: [2.81270435]	 Cost: 14.815895759150004
Iteration: 94	 Weight: [9.79872665]	 Bias: [2.81009445]	 Cost: 14.815547353948054
Iteration: 95	 Weight: [9.79913568]	 Bias: [2.80749347]	 Cost: 14.81520132811472
Iteration: 96	 Weight: [9.7995433]	 Bias: [2.80490139]	 Cost: 14.814857665400533
Iteration: 97	 Weight: [9.79994953]	 Bias: [2.80231818]	 Cost: 14.814516349667015
Iteration: 98	 Weight: [9.80035438]	 Bias: [2.79974381]	 Cost: 14.814177364885879
Iteration: 99	 Weight: [9.80075783]	 Bias: [2.79717824]	 Cost: 14.813840695138357
Iteration: 100	 Weight: [9.80115991]	 Bias: [2.79462144]	 Cost: 14.81350632461434
Iteration: 101	 Weight: [9.80156061]	 Bias: [2.79207339]	 Cost: 14.813174237611687
Iteration: 102	 Weight: [9.80195994]	 Bias: [2.78953406]	 Cost: 14.812844418535544
Iteration: 103	 Weight: [9.80235791]	 Bias: [2.78700341]	 Cost: 14.81251685189751
Iteration: 104	 Weight: [9.80275451]	 Bias: [2.78448142]	 Cost: 14.81219152231498
Iteration: 105	 Weight: [9.80314976]	 Bias: [2.78196805]	 Cost: 14.811868414510409
Iteration: 106	 Weight: [9.80354365]	 Bias: [2.77946328]	 Cost: 14.811547513310554
Iteration: 107	 Weight: [9.8039362]	 Bias: [2.77696708]	 Cost: 14.811228803645841
Iteration: 108	 Weight: [9.8043274]	 Bias: [2.77447942]	 Cost: 14.810912270549595
Iteration: 109	 Weight: [9.80471727]	 Bias: [2.77200026]	 Cost: 14.810597899157315
Iteration: 110	 Weight: [9.8051058]	 Bias: [2.76952959]	 Cost: 14.810285674706066
Iteration: 111	 Weight: [9.80549301]	 Bias: [2.76706737]	 Cost: 14.809975582533701
Iteration: 112	 Weight: [9.80587889]	 Bias: [2.76461357]	 Cost: 14.809667608078241
Iteration: 113	 Weight: [9.80626345]	 Bias: [2.76216816]	 Cost: 14.809361736877108
Iteration: 114	 Weight: [9.80664669]	 Bias: [2.75973112]	 Cost: 14.80905795456651
Iteration: 115	 Weight: [9.80702863]	 Bias: [2.75730241]	 Cost: 14.808756246880783
Iteration: 116	 Weight: [9.80740925]	 Bias: [2.75488201]	 Cost: 14.808456599651663
Iteration: 117	 Weight: [9.80778858]	 Bias: [2.75246989]	 Cost: 14.80815899880762
Iteration: 118	 Weight: [9.80816661]	 Bias: [2.75006602]	 Cost: 14.807863430373262
Iteration: 119	 Weight: [9.80854334]	 Bias: [2.74767037]	 Cost: 14.807569880468618
Iteration: 120	 Weight: [9.80891879]	 Bias: [2.74528292]	 Cost: 14.807278335308524
Iteration: 121	 Weight: [9.80929295]	 Bias: [2.74290363]	 Cost: 14.806988781201952
Iteration: 122	 Weight: [9.80966583]	 Bias: [2.74053248]	 Cost: 14.806701204551349
Iteration: 123	 Weight: [9.81003744]	 Bias: [2.73816945]	 Cost: 14.806415591852064
Iteration: 124	 Weight: [9.81040778]	 Bias: [2.73581449]	 Cost: 14.806131929691658
Iteration: 125	 Weight: [9.81077684]	 Bias: [2.73346759]	 Cost: 14.80585020474928
Iteration: 126	 Weight: [9.81114465]	 Bias: [2.73112872]	 Cost: 14.805570403795063
Iteration: 127	 Weight: [9.8115112]	 Bias: [2.72879784]	 Cost: 14.805292513689489
Iteration: 128	 Weight: [9.81187649]	 Bias: [2.72647495]	 Cost: 14.805016521382774
Iteration: 129	 Weight: [9.81224054]	 Bias: [2.72415999]	 Cost: 14.80474241391425
Iteration: 130	 Weight: [9.81260334]	 Bias: [2.72185296]	 Cost: 14.804470178411758
Iteration: 131	 Weight: [9.8129649]	 Bias: [2.71955381]	 Cost: 14.80419980209107
Iteration: 132	 Weight: [9.81332522]	 Bias: [2.71726253]	 Cost: 14.803931272255236
Iteration: 133	 Weight: [9.81368431]	 Bias: [2.71497909]	 Cost: 14.80366457629403
Iteration: 134	 Weight: [9.81404217]	 Bias: [2.71270346]	 Cost: 14.803399701683336
Iteration: 135	 Weight: [9.81439881]	 Bias: [2.71043561]	 Cost: 14.803136635984613
Iteration: 136	 Weight: [9.81475423]	 Bias: [2.70817552]	 Cost: 14.80287536684421
Iteration: 137	 Weight: [9.81510843]	 Bias: [2.70592315]	 Cost: 14.802615881992867
Iteration: 138	 Weight: [9.81546142]	 Bias: [2.7036785]	 Cost: 14.802358169245114
Iteration: 139	 Weight: [9.8158132]	 Bias: [2.70144152]	 Cost: 14.8021022164987
Iteration: 140	 Weight: [9.81616378]	 Bias: [2.69921219]	 Cost: 14.801848011734023
Iteration: 141	 Weight: [9.81651316]	 Bias: [2.69699049]	 Cost: 14.801595543013562
Iteration: 142	 Weight: [9.81686135]	 Bias: [2.69477639]	 Cost: 14.801344798481344
Iteration: 143	 Weight: [9.81720834]	 Bias: [2.69256986]	 Cost: 14.801095766362307
Iteration: 144	 Weight: [9.81755415]	 Bias: [2.69037088]	 Cost: 14.800848434961877
Iteration: 145	 Weight: [9.81789877]	 Bias: [2.68817942]	 Cost: 14.800602792665305
Iteration: 146	 Weight: [9.81824222]	 Bias: [2.68599545]	 Cost: 14.80035882793717
Iteration: 147	 Weight: [9.81858449]	 Bias: [2.68381896]	 Cost: 14.800116529320814
Iteration: 148	 Weight: [9.81892559]	 Bias: [2.68164991]	 Cost: 14.79987588543787
Iteration: 149	 Weight: [9.81926553]	 Bias: [2.67948828]	 Cost: 14.799636884987612
Weight: [9.81926553] Bias: [2.67948828]
# Clone the entire repo.
!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo
%cd cloned-repo
!ls
Cloning into 'cloned-repo'...
warning: --local is ignored
remote: Enumerating objects: 1813, done.
remote: Counting objects: 100% (120/120), done.
remote: Compressing objects: 100% (100/100), done.
remote: Total 1813 (delta 30), reused 76 (delta 20), pack-reused 1693
Receiving objects: 100% (1813/1813), 34.24 MiB | 25.65 MiB/s, done.
Resolving deltas: 100% (1128/1128), done.
/content/cloned-repo/cloned-repo/cloned-repo/cloned-repo/cloned-repo
environment.yml  LICENSE-TEXT  README.md	 tools
LICENSE-CODE	 notebooks     requirements.txt  website
# Fetch a single <1MB file using the raw GitHub URL.
!curl --remote-name \
     -H 'Accept: application/vnd.github.v3.raw' \
     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 59658  100 59658    0     0   147k      0 --:--:-- --:--:-- --:--:--  146k
w = regressor.weight
b = regressor.bias
x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8] 
y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]
plt.scatter(x,y)
axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = b + w * x_vals
plt.plot(x_vals, y_vals)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
